{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "\n",
    "Using meteorological data from ERA5 dataset. Output unscaled.  \n",
    "Filling nan values with nearest data point.   \n",
    "Train for T time-stamps moving window and predict at $T^{th}$ time-stamp.   \n",
    "initialize weights : ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9vke4wxgIJ6"
   },
   "outputs": [],
   "source": [
    "!pip install -qq torchsummaryX\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "from torchsummaryX import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "torch.cuda.is_available()\n",
    "import multiprocessing as mp\n",
    "import resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b9kwB8nt0c5_"
   },
   "source": [
    "## Defining the master class\n",
    "\n",
    "### few terminologies\n",
    "\n",
    "Kindly refer to ADAIN model figure to see right and left parts of network.\n",
    "\n",
    "right_*, *_R* = right part of the network (example, right_timeseries_data, lstm_R1)   \n",
    "left_*, *_L* = left part of the network (example, left_timeseries_data, fc_L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aWnF4moXJVjM"
   },
   "outputs": [],
   "source": [
    "class ADAIN(nn.Module):\n",
    "  def __init__(self, init_data):\n",
    "    super(ADAIN, self).__init__()\n",
    "    ##############################\n",
    "    # ALL Metadata\n",
    "    ##############################\n",
    "    met_dim, time_window, drop_out = [i.item() for i in init_data]\n",
    "    # met_dim = Number of meteorogical features\n",
    "    # time_window = number of previous time-stamps to leverage for predictions\n",
    "    # drop_out = drop out in each layer\n",
    "    drop_out = drop_out/100 # convert percentage to decimal\n",
    "    self.drop_out = drop_out\n",
    "\n",
    "    ########## Defining the architecture ################\n",
    "    ##############################\n",
    "    # For Left station (target station) \n",
    "    ##############################\n",
    "    self.lstm_L1 = nn.LSTM(input_size=met_dim, hidden_size=300, batch_first=True, dropout=drop_out)\n",
    "    self.fc_L2 = nn.Linear(in_features = 300, out_features = 200)\n",
    "\n",
    "    ##############################\n",
    "    # For Right stations (train stations)\n",
    "    ##############################\n",
    "    self.fc_R1 = nn.Linear(in_features = 2, out_features = 100) # in_feature = 2 for 2D distance\n",
    "    self.lstm_R1 = nn.LSTM(input_size = met_dim+1, hidden_size = 300, batch_first=True, dropout=drop_out) \n",
    "    # met_dim + 1 => meteorology_dimention + aqi_dimention\n",
    "    self.fc_R2 = nn.Linear(in_features= 400, out_features = 200)\n",
    "    self.fc_R3 = nn.Linear(in_features = 200, out_features = 200)\n",
    "\n",
    "    ##############################\n",
    "    # Attention part\n",
    "    ##############################\n",
    "    self.fcA1 = nn.Linear(in_features = 400, out_features = 200)\n",
    "    self.fcA2 = nn.Linear(in_features = 200, out_features = 1)\n",
    "\n",
    "    ##############################\n",
    "    # Fusion Layer\n",
    "    ##############################\n",
    "    self.fc_final1 = nn.Linear(in_features = 400, out_features = 200)\n",
    "    self.fc_final2 = nn.Linear(in_features = 200, out_features = 1)\n",
    "\n",
    "  def forward_once(self, s_id, right_timeseries_data, right_static_data, left_output):\n",
    "    #########################\n",
    "    # This method does one forward pass over recurring part of the right part of network \n",
    "    #########################\n",
    "    \n",
    "    # s_id = station index in right_timeseries_data & right_static_data\n",
    "    # right_timeseries_data -> shape = (-1, time_window, n_train_stations, met_dim+1)\n",
    "    # right_static_data -> shape = (-1, n_train_stations, 2) # 2 for 2d distance\n",
    "    # left_output -> shape = (-1, 200)\n",
    "    \n",
    "    # Right LSTM\n",
    "    lstm_out, _ = self.lstm_R1(right_timeseries_data[:,:,s_id,:]) # out_shape = (-1, time_window, 300)\n",
    "    lstm_out = lstm_out[:,-1,:] # taking last hidden state        # out_shape = (-1, 300)\n",
    "    \n",
    "    # Right FC\n",
    "    fc_out = self.fc_R1(right_static_data[:,s_id,:])                \n",
    "    fc_out = F.dropout(F.relu(fc_out), self.drop_out, training=self.training)\n",
    "                                                                  # out_shape = (-1, 100)\n",
    "        \n",
    "    # Combine in FC\n",
    "    combined_fc = torch.cat((fc_out, lstm_out), dim=1)            # out_shape = (-1, time_features+100)\n",
    "    \n",
    "    combined_fc = self.fc_R2(combined_fc)                         # out_shape = (-1, self.n_train_stations, 200)\n",
    "    combined_fc = F.dropout(F.relu(combined_fc), self.drop_out, training=self.training)\n",
    "    combined_fc = self.fc_R3(combined_fc)                         # out_shape = (-1, 200)        \n",
    "    right_output = F.dropout(F.relu(combined_fc), self.drop_out, training=self.training) \n",
    "                                                                  # out_shape = (-1, 200)\n",
    "    ##############################\n",
    "    # Attention Forward\n",
    "    ##############################\n",
    "    attention = torch.cat((left_output, right_output), dim=1)     # out_shape = (-1, 400)\n",
    "    attention = F.relu(self.fcA1(attention))                      # out_shape = (-1, 200)\n",
    "    attention = self.fcA2(attention).view(-1,1,1)                 # out_shape = (-1, 1, 1)\n",
    "    return attention, right_output.view(-1,1,200)                    # out_shape = (-1, 1, 1), (-1,1,200)\n",
    "\n",
    "  def forward(self, left_timeseries, right_static_data, right_timeseries_data):\n",
    "    ##############################\n",
    "    # This method is main forward method of the class\n",
    "    ##############################\n",
    "    \n",
    "    ##############################\n",
    "    # Left forward\n",
    "    ##############################\n",
    "    \n",
    "    # (-1,*,*)-> -1 in beginning of the shape is batch_size throughout this code\n",
    "    \n",
    "    # Left lstm\n",
    "    lstm_out, _ = self.lstm_L1(left_timeseries)                   # out_shape = (-1, self.time_window, 300)\n",
    "    lstm_out = lstm_out[:,-1,:] # taking last hidden state        # out_shape = (-1, 300)\n",
    "\n",
    "    # Left higher FC (ommiting lower FC as we don't have POI data)\n",
    "    fc_out = self.fc_L2(lstm_out)\n",
    "    left_output = F.relu(fc_out)\n",
    "\n",
    "    ##############################\n",
    "    # Right & Attention forward\n",
    "    ##############################\n",
    "    for s_id in range(right_timeseries_data.shape[2]):\n",
    "        attention, right_output = self.forward_once(s_id, right_timeseries_data, right_static_data, left_output)\n",
    "    \n",
    "    attention_softmax = attention/attention.sum(axis=1).view(-1,1,1) # out_shape = (-1, 20, 1)\n",
    "    attention_output = (right_output * attention_softmax).sum(axis=1) # sum((-1,20,1) * (-1,20,1)) ==> (-1, 1)\n",
    "    \n",
    "    ##############################\n",
    "    # Final fusion forward\n",
    "    ##############################\n",
    "    \n",
    "    fusion_output = self.fc_final1(torch.cat((attention_output, left_output), dim=1)) # (-1, 200)\n",
    "    fusion_output = F.dropout(F.relu(fusion_output), self.drop_out, training=self.training)\n",
    "    fusion_output = self.fc_final2(fusion_output) # (-1,200)->(-1, 1)\n",
    "    return fusion_output.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ADAIN(init_data.to('cuda')).to('cuda')\n",
    "# print(summary(model, *[i.to(device) for i in train_dataloaders[left_station][batch][:3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4wAOHeq6KYJW"
   },
   "source": [
    "### Let's go to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "met_dim = 11\n",
    "n_train_stations=21\n",
    "n_val_stations=3\n",
    "n_test_stations=12\n",
    "time_window = 24\n",
    "n_folds = 3\n",
    "regularizer = 0.01\n",
    "lr = 0.01\n",
    "device='cuda'\n",
    "n_epochs = 5\n",
    "drop_out = 50 # in percentage\n",
    "\n",
    "###\n",
    "t_start = 0\n",
    "t_end = 24*10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation Fold 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing test data for  11 1036.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0) # to control the shuffle in train data\n",
    "f_n = 0\n",
    "\n",
    "data_dict = pd.read_pickle('../Data/ERA5/train_val_test.pickle')\n",
    "ignore_cols = ['name_chinese','name_english','district_id']\n",
    "latlon_data = pd.read_csv('../Data/raw/Beijing_station.csv').set_index('station_id').sort_index().drop(columns=ignore_cols)\n",
    "t_steps = data_dict[f_n]['train'].index.unique().sort_values()[t_start:t_end]\n",
    "\n",
    "#######################\n",
    "# Data prep\n",
    "#######################\n",
    "train_dataloaders = {}\n",
    "val_dataloaders = {}\n",
    "test_dataloaders = {}\n",
    "\n",
    "train_data = data_dict[f_n]['train']\n",
    "val_data = data_dict[f_n]['val']\n",
    "test_data = data_dict[f_n]['test']\n",
    "\n",
    "train_stations = train_data.station_id.unique().tolist()\n",
    "val_stations = val_data.station_id.unique().tolist()\n",
    "test_stations = test_data.station_id.unique().tolist()\n",
    "\n",
    "##############################################\n",
    "# Train data preperation\n",
    "##############################################\n",
    "for s_id, left_station in enumerate(train_data.station_id.unique()):\n",
    "    clear_output(wait=True)\n",
    "    print('preparing train data for ', s_id, left_station)\n",
    "    \n",
    "    right_stations = train_data.station_id.unique()\n",
    "    right_stations = np.setdiff1d(right_stations, left_station)\n",
    "    drop_cols = ['station_id','PM25','latitude','longitude','time']\n",
    "    left_timeseries_data = train_data[train_data.station_id==left_station].drop(columns=drop_cols)\n",
    "    left_aqi_data = train_data[train_data.station_id==left_station]['PM25']\n",
    "    \n",
    "    # Initialize empty arrays\n",
    "    left_timeseries = np.zeros(((len(t_steps)-time_window+1), time_window, met_dim), dtype=np.float32)*np.nan\n",
    "    left_aqi = np.zeros(((len(t_steps)-time_window+1), 1), dtype=np.float32)*np.nan\n",
    "    ###################################\n",
    "    \n",
    "    drop_cols.remove('PM25')\n",
    "    right_timeseries_data = train_data[train_data.station_id.isin(right_stations)].drop(columns=drop_cols)\n",
    "    right_timeseries_data = np.stack(np.split(right_timeseries_data.values, \n",
    "                                               indices_or_sections=len(right_stations)))\n",
    "    right_timeseries_data = np.moveaxis(right_timeseries_data, 0, 1)\n",
    "    \n",
    "    # Initialize empty arrays\n",
    "    right_timeseries = np.nan * np.zeros(((len(t_steps)-time_window+1), \n",
    "                                           time_window, \n",
    "                                           len(right_stations), \n",
    "                                           met_dim+1), dtype=np.float32)\n",
    "    ################################\n",
    "    \n",
    "    right_dis_data = (train_data[train_data.station_id.isin(right_stations)]\\\n",
    "    .set_index('station_id')[['latitude','longitude']].drop_duplicates().values-\\\n",
    "    train_data[train_data.station_id==left_station]\\\n",
    "    .set_index('station_id')[['latitude','longitude']].drop_duplicates().values).astype(np.float32)\n",
    "\n",
    "    right_dis = np.zeros(((len(t_steps)-time_window+1), len(right_stations), 2), \n",
    "                             dtype=np.float32)*np.nan\n",
    "\n",
    "    for t in range(len(t_steps)-time_window+1):\n",
    "        left_timeseries[t,:,:] = left_timeseries_data[str(t_steps[t]):str(t_steps[t+time_window-1])]\n",
    "        left_aqi[t,:] = left_aqi_data[str(t_steps[t+time_window-1])]\n",
    "        right_timeseries[t,:,:,:] = right_timeseries_data[t:t+time_window]\n",
    "        right_dis[t,:,:] = right_dis_data\n",
    "    dataset = TensorDataset(torch.tensor(left_timeseries), \n",
    "                            torch.tensor(right_dis), \n",
    "                            torch.tensor(right_timeseries),\n",
    "                            torch.tensor(left_aqi))\n",
    "    train_dataloaders[left_station] = list(DataLoader(dataset, batch_size=batch_size, shuffle=True))\n",
    "\n",
    "    \n",
    "##############################################\n",
    "# Validation data preperation\n",
    "##############################################\n",
    "for s_id, left_station in enumerate(val_stations):\n",
    "    clear_output(wait=True)\n",
    "    print('preparing val data for ', s_id, left_station)\n",
    "    drop_cols = ['station_id','PM25','latitude','longitude','time']\n",
    "    left_timeseries_data = val_data[val_data.station_id==left_station].drop(columns=drop_cols)\n",
    "    left_aqi_data = val_data[val_data.station_id==left_station]['PM25']\n",
    "    \n",
    "    left_timeseries = np.zeros(((len(t_steps)-time_window+1), time_window, met_dim), dtype=np.float32)*np.nan\n",
    "    left_aqi = np.zeros(((len(t_steps)-time_window+1), 1), dtype=np.float32)*np.nan\n",
    "\n",
    "    drop_cols.remove('PM25')\n",
    "    right_timeseries_data = train_data.drop(columns=drop_cols)\n",
    "    right_timeseries_data = np.stack(np.split(right_timeseries_data.values, \n",
    "                                               indices_or_sections=len(right_stations)))\n",
    "    right_timeseries_data = np.moveaxis(right_timeseries_data, 0, 1)\n",
    "    right_timeseries = np.nan * np.zeros(((len(t_steps)-time_window+1), \n",
    "                                           time_window, \n",
    "                                           len(right_stations), \n",
    "                                           met_dim+1), dtype=np.float32)\n",
    "\n",
    "    right_dis_data = (train_data\\\n",
    "    .set_index('station_id')[['latitude','longitude']].drop_duplicates().values-\\\n",
    "    val_data[val_data.station_id==left_station]\\\n",
    "    .set_index('station_id')[['latitude','longitude']].drop_duplicates().values).astype(np.float32)\n",
    "\n",
    "    right_dis = np.zeros(((len(t_steps)-time_window+1), len(train_stations), 2), \n",
    "                             dtype=np.float32)*np.nan\n",
    "\n",
    "    for t in range(len(t_steps)-time_window+1):\n",
    "        left_timeseries[t,:,:] = left_timeseries_data[str(t_steps[t]):str(t_steps[t+time_window-1])]\n",
    "        left_aqi[t,:] = left_aqi_data[str(t_steps[t+time_window-1])]\n",
    "        right_timeseries[t,:,:,:] = right_timeseries_data[t:t+time_window]\n",
    "        right_dis[t,:,:] = right_dis_data\n",
    "    dataset = TensorDataset(torch.tensor(left_timeseries), \n",
    "                            torch.tensor(right_dis), \n",
    "                            torch.tensor(right_timeseries),\n",
    "                            torch.tensor(left_aqi))\n",
    "    val_dataloaders[left_station] = list(DataLoader(dataset, batch_size=batch_size))\n",
    "\n",
    "##############################################\n",
    "# Test data preperation\n",
    "##############################################\n",
    "for s_id, left_station in enumerate(test_stations):\n",
    "    clear_output(wait=True)\n",
    "    print('preparing test data for ', s_id, left_station)\n",
    "    drop_cols = ['station_id','PM25','latitude','longitude','time']\n",
    "    left_timeseries_data = test_data[test_data.station_id==left_station].drop(columns=drop_cols)\n",
    "    left_aqi_data = test_data[test_data.station_id==left_station]['PM25']\n",
    "    \n",
    "    left_timeseries = np.zeros(((len(t_steps)-time_window+1), time_window, met_dim), dtype=np.float32)*np.nan\n",
    "    left_aqi = np.zeros(((len(t_steps)-time_window+1), 1), dtype=np.float32)*np.nan\n",
    "\n",
    "    drop_cols.remove('PM25')\n",
    "    right_timeseries_data = train_data.drop(columns=drop_cols)\n",
    "    right_timeseries_data = np.stack(np.split(right_timeseries_data.values, \n",
    "                                               indices_or_sections=len(right_stations)))\n",
    "    right_timeseries_data = np.moveaxis(right_timeseries_data, 0, 1)\n",
    "    right_timeseries = np.nan * np.zeros(((len(t_steps)-time_window+1), \n",
    "                                           time_window, \n",
    "                                           len(right_stations), \n",
    "                                           met_dim+1), dtype=np.float32)\n",
    "\n",
    "    right_dis_data = (train_data\\\n",
    "    .set_index('station_id')[['latitude','longitude']].drop_duplicates().values-\\\n",
    "    test_data[test_data.station_id==left_station]\\\n",
    "    .set_index('station_id')[['latitude','longitude']].drop_duplicates().values).astype(np.float32)\n",
    "\n",
    "    right_dis = np.zeros(((len(t_steps)-time_window+1), len(train_stations), 2), \n",
    "                             dtype=np.float32)*np.nan\n",
    "\n",
    "    for t in range(len(t_steps)-time_window+1):\n",
    "        left_timeseries[t,:,:] = left_timeseries_data[str(t_steps[t]):str(t_steps[t+time_window-1])]\n",
    "        left_aqi[t,:] = left_aqi_data[str(t_steps[t+time_window-1])]\n",
    "        right_timeseries[t,:,:,:] = right_timeseries_data[t:t+time_window]\n",
    "        right_dis[t,:,:] = right_dis_data\n",
    "    dataset = TensorDataset(torch.tensor(left_timeseries), \n",
    "                            torch.tensor(right_dis), \n",
    "                            torch.tensor(right_timeseries),\n",
    "                            torch.tensor(left_aqi))\n",
    "    test_dataloaders[left_station] = list(DataLoader(dataset, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Fold 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cOdfzGbpKerq",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train f_n= 0 epoch= 0 1001.0 loss= 609.156494140625 batch= 8 of 10 out tensor([29.0463, 44.1916, 43.3254, 65.4282, 51.3721, 37.2432, 35.4954, 49.4398,\n",
      "        48.9868, 45.2009, 48.6309, 54.2246, 44.1604, 36.6907, 52.0210, 51.1292,\n",
      "        41.3338, 50.1381, 39.6441, 38.7214, 45.9214, 46.6660, 44.6516, 47.4464]) grn tensor([[ 65.],\n",
      "        [ 31.],\n",
      "        [ 29.],\n",
      "        [ 75.],\n",
      "        [ 62.],\n",
      "        [ 34.],\n",
      "        [ 90.],\n",
      "        [ 62.],\n",
      "        [ 19.],\n",
      "        [ 26.],\n",
      "        [ 29.],\n",
      "        [ 77.],\n",
      "        [ 36.],\n",
      "        [ 61.],\n",
      "        [ 62.],\n",
      "        [105.],\n",
      "        [ 57.],\n",
      "        [ 77.],\n",
      "        [ 57.],\n",
      "        [ 62.],\n",
      "        [ 57.],\n",
      "        [ 37.],\n",
      "        [ 20.],\n",
      "        [ 54.]])\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# Model definition\n",
    "#######################\n",
    "init = time()\n",
    "torch.manual_seed(0)\n",
    "global_loss = np.inf\n",
    "init_data = torch.tensor([met_dim, time_window, drop_out])\n",
    "ADAINmodel = ADAIN(init_data).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "val_criterion = nn.MSELoss()\n",
    "ADAINoptimizer = optim.Adam(ADAINmodel.parameters(), lr=lr, weight_decay=regularizer)\n",
    "# ADAINoptimizer = optim.SGD(ADAINmodel.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "# Initialize weights\n",
    "def weights_init_uniform(model):\n",
    "    for p in model.parameters():\n",
    "        torch.nn.init.uniform_(p, -0.1, 0.1)\n",
    "weights_init_uniform(ADAINmodel)\n",
    "\n",
    "### Train + Validation #############\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    #################################\n",
    "    # Training\n",
    "    #################################\n",
    "    ADAINmodel.train() # Enable train mode\n",
    "\n",
    "    out = []\n",
    "    grn = []\n",
    "    n_batch = len(train_dataloaders[list(train_dataloaders.keys())[0]])\n",
    "    for batch in range(n_batch):\n",
    "        ADAINoptimizer.zero_grad()\n",
    "        for left_station in train_data.station_id.unique():\n",
    "            # forward + backward + optimize\n",
    "            outputs_t = ADAINmodel(*[i.to(device) for i in train_dataloaders[left_station][batch][:3]])\n",
    "            loss = criterion(outputs_t, train_dataloaders[left_station][batch][3].to(device))\n",
    "            out.append(outputs_t.cpu().detach())\n",
    "            grn.append(train_dataloaders[left_station][batch][3])\n",
    "            loss.backward()\n",
    "            clear_output(wait=True)\n",
    "            print('train','f_n=',f_n,'epoch=', epoch, left_station, 'loss=',loss.item(), \n",
    "                  'batch=',batch,'of',n_batch, 'out',out[-1],'grn',grn[-1])\n",
    "        ADAINoptimizer.step()\n",
    "    print('Training over for epoch',epoch)\n",
    "    pd.to_pickle({'fold'+str(f_n): {'out':np.concatenate(out).squeeze(), 'grn':np.concatenate(grn).squeeze()}}, \n",
    "                 '../Results/test_ADAIN_fold_'+str(f_n)+'values.pickle')\n",
    "    pd.to_pickle(np.sqrt(np.mean((np.concatenate(out).squeeze()- np.concatenate(grn).squeeze())**2))\n",
    "                 , '../Results/'+str(epoch).zfill(4)+'trn_ADAIN_'+str(f_n)+'_fold.pickle')\n",
    "\n",
    "    #################################\n",
    "    # Validation\n",
    "    #################################\n",
    "    ADAINmodel.eval()\n",
    "    print('started validation',epoch)\n",
    "    out = []\n",
    "    grn = []\n",
    "    for batch in range(len(val_dataloaders[list(val_dataloaders.keys())[0]])):\n",
    "        for val_station in val_data.station_id.unique():\n",
    "            outputs_v = ADAINmodel(*[i.to(device) for i in val_dataloaders[val_station][batch][:3]])\n",
    "            out.append(outputs_v.cpu().detach())\n",
    "            grn.append(val_dataloaders[val_station][batch][3])\n",
    "            loss = criterion(outputs_v, val_dataloaders[val_station][batch][3].to(device))\n",
    "            clear_output(wait=True)\n",
    "            print('f_n',f_n, epoch, val_station, loss.item(), batch)\n",
    "    epoch_loss = np.sqrt(np.mean((np.concatenate(out).squeeze()- np.concatenate(grn).squeeze())**2))\n",
    "    if epoch_loss < global_loss:\n",
    "        global_loss = epoch_loss\n",
    "        torch.save(ADAINmodel.state_dict(), '../models/'+str(f_n)+'fold_ADAIN.h5')\n",
    "    pd.to_pickle(epoch_loss, '../Results/'+str(epoch).zfill(4)+'val_ADAIN_'+str(f_n)+'_fold.pickle')\n",
    "\n",
    "#################################\n",
    "# Testing\n",
    "#################################\n",
    "test_model = ADAIN(init_data).to(device)\n",
    "test_model.load_state_dict(torch.load('../models/'+str(f_n)+'fold_ADAIN.h5'))\n",
    "test_model.eval()\n",
    "print('started test')\n",
    "out = []\n",
    "grn = []\n",
    "for batch in range(len(test_dataloaders[list(test_dataloaders.keys())[0]])):\n",
    "    for test_station in test_data.station_id.unique():\n",
    "        outputs_v = test_model(*[i.to(device) for i in test_dataloaders[test_station][batch][:3]])\n",
    "        out.append(outputs_v.cpu().detach())\n",
    "        grn.append(val_dataloaders[val_station][batch][3])\n",
    "        loss = criterion(outputs_v, test_dataloaders[test_station][batch][3].to(device))\n",
    "        clear_output(wait=True)\n",
    "        print('f_n',f_n,test_station, loss.item(), batch)\n",
    "pd.to_pickle({'fold'+str(f_n): {'out':np.concatenate(out).squeeze(), 'grn':np.concatenate(grn).squeeze()}}, '../Results/test_ADAIN_fold_'+str(f_n)+'values.pickle')\n",
    "pd.to_pickle(np.sqrt(np.mean((np.concatenate(out).squeeze()- np.concatenate(grn).squeeze())**2)), '../Results/test_ADAIN_fold'+str(f_n)+'.pickle')\n",
    "print(time()-init, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.564587\n",
      "32.483482\n",
      "31.850136\n",
      "31.705538\n",
      "31.827078\n"
     ]
    }
   ],
   "source": [
    "## Train loss\n",
    "for i in range(n_epochs):\n",
    "    print(pd.read_pickle('../Results/'+str(i).zfill(4)+'trn_ADAIN_'+str(f_n)+'_fold.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.826324\n",
      "24.656431\n",
      "23.86893\n",
      "24.47673\n",
      "21.965303\n"
     ]
    }
   ],
   "source": [
    "## Validation loss\n",
    "for i in range(n_epochs):\n",
    "    print(pd.read_pickle('../Results/'+str(i).zfill(4)+'val_ADAIN_'+str(f_n)+'_fold.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.044458"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test loss\n",
    "pd.read_pickle('../Results/test_ADAIN_fold'+str(f_n)+'.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grn, out\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(62.0, 50.62975311279297),\n",
       " (70.0, 50.589359283447266),\n",
       " (72.0, 50.215179443359375),\n",
       " (65.0, 49.79447937011719),\n",
       " (62.0, 49.249534606933594),\n",
       " (53.0, 48.84075164794922),\n",
       " (54.0, 48.80052947998047),\n",
       " (59.0, 48.88352584838867),\n",
       " (55.0, 49.04583740234375),\n",
       " (56.0, 49.50800323486328),\n",
       " (63.0, 49.819602966308594),\n",
       " (66.0, 49.43227767944336),\n",
       " (74.0, 49.9012565612793),\n",
       " (62.0, 50.15349578857422),\n",
       " (61.0, 50.17626953125),\n",
       " (60.0, 50.11144256591797),\n",
       " (40.0, 49.770362854003906),\n",
       " (28.0, 49.849403381347656),\n",
       " (31.0, 50.25349807739258),\n",
       " (34.0, 50.52042007446289),\n",
       " (34.0, 50.44000244140625),\n",
       " (39.0, 50.47332000732422),\n",
       " (44.0, 50.410545349121094),\n",
       " (49.0, 50.12386703491211)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = 56\n",
    "# Train\n",
    "\n",
    "# val\n",
    "\n",
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTHOR RMSE IS 49.45"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ADAIN Implementation-V1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
